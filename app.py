"""
Cross-Border Payment Fraud Detection System
è·¨å¢ƒæ”¯ä»˜æ¬ºè©æª¢æ¸¬ç³»çµ± - Streamlit Dashboard

AIæ™ºæ…§ç¤¾æœƒç”±æ‚¨å‰µ - æ¾³é–€é›»è¨ŠAI+å¤§æ•¸æ“šæ™ºæ…§æ‡‰ç”¨è¨­è¨ˆæ¯”è³½
"""
import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
from datetime import datetime, timedelta
import json

from models.fraud_detector import EnsembleFraudDetector
from models.behavioral_biometrics import BehavioralBiometrics
from models.deepfake_detector import DeepfakeDetector
from models.hybrid_ai_system import HybridAISystem
from utils.graph_analyzer import MoneyLaunderingDetector
from utils.federated_learning import FederatedLearning
from utils.data_loader import load_transactions, load_user_profiles, validate_data
from config import (BANKS, FRAUD_THRESHOLD, BEHAVIORAL_THRESHOLD, 
                    RECENT_TRANSACTIONS_COUNT, CHART_HEIGHT, TABLE_HEIGHT,
                    CURRENCY, BANK_CODES)

# Page configuration
st.set_page_config(
    page_title="è·¨å¢ƒæ”¯ä»˜æ¬ºè©æª¢æ¸¬ç³»çµ±",
    page_icon="ğŸ›¡ï¸",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
    .reportview-container .main .block-container {
        padding-top: 2rem;
        padding-bottom: 2rem;
    }
    .stMetric {
        background-color: #1e1e1e !important;
        padding: 15px;
        border-radius: 8px;
        border: 1px solid #404040;
        box-shadow: 0 2px 8px rgba(0,0,0,0.3);
    }
    .stMetric > div {
        color: #ffffff !important;
    }
    .stMetric [data-testid="metric-label"] {
        color: #ffffff !important;
        font-weight: 600;
    }
    .stMetric [data-testid="metric-value"] {
        color: #ffffff !important;
        font-weight: 700;
        font-size: 1.2em;
    }
    .stMetric [data-testid="metric-delta"] {
        color: #4ade80 !important;
        font-weight: 600;
    }
    h1 {
        color: #1f77b4;
    }
    .fraud-alert {
        background-color: #ffebee;
        padding: 10px;
        border-radius: 5px;
        border-left: 5px solid #f44336;
    }
    .success-alert {
        background-color: #e8f5e9;
        padding: 10px;
        border-radius: 5px;
        border-left: 5px solid #4caf50;
    }
    
    /* Ensure all text is visible */
    .main .block-container {
        color: #262730;
    }
    
    /* Fix metric containers specifically */
    div[data-testid="metric-container"] {
        background-color: #1e1e1e !important;
        border: 1px solid #404040 !important;
        padding: 1rem !important;
        border-radius: 0.5rem !important;
        box-shadow: 0 2px 8px rgba(0,0,0,0.3) !important;
    }
    
    div[data-testid="metric-container"] > div {
        color: #ffffff !important;
    }
    
    /* Ensure proper contrast for all text */
    .stText, .stMarkdown, .stMetric {
        color: #ffffff !important;
    }
    
    /* Additional metric styling for better visibility */
    [data-testid="metric-container"] {
        background: linear-gradient(135deg, #1e1e1e 0%, #2d2d2d 100%) !important;
    }
    
    [data-testid="metric-container"] [data-testid="metric-label"] {
        color: #e0e0e0 !important;
        font-size: 0.9em !important;
    }
    
    [data-testid="metric-container"] [data-testid="metric-value"] {
        color: #ffffff !important;
        font-size: 1.4em !important;
        font-weight: 800 !important;
    }
    
    [data-testid="metric-container"] [data-testid="metric-delta"] {
        color: #4ade80 !important;
        font-weight: 700 !important;
    }
    
    /* Fix table styling for better visibility */
    .stDataFrame {
        background-color: #1e1e1e !important;
        color: #ffffff !important;
    }
    
    .stDataFrame table {
        background-color: #1e1e1e !important;
        color: #ffffff !important;
    }
    
    .stDataFrame th {
        background-color: #2d2d2d !important;
        color: #ffffff !important;
        border: 1px solid #404040 !important;
    }
    
    .stDataFrame td {
        background-color: #1e1e1e !important;
        color: #ffffff !important;
        border: 1px solid #404040 !important;
    }
    
    .stDataFrame tr:nth-child(even) {
        background-color: #2a2a2a !important;
    }
    
    .stDataFrame tr:nth-child(odd) {
        background-color: #1e1e1e !important;
    }
    
    /* Fix dataframe container */
    div[data-testid="stDataFrame"] {
        background-color: #1e1e1e !important;
        border: 1px solid #404040 !important;
        border-radius: 8px !important;
    }
    
    /* Fix any remaining text elements */
    .stTable {
        background-color: #1e1e1e !important;
        color: #ffffff !important;
    }
    
    .stTable table {
        background-color: #1e1e1e !important;
        color: #ffffff !important;
    }
</style>
""", unsafe_allow_html=True)

# Initialize session state
@st.cache_resource
def initialize_models():
    """Initialize all models (cached)"""
    return {
        'fraud_detector': EnsembleFraudDetector(),
        'behavioral_analyzer': BehavioralBiometrics(),
        'deepfake_detector': DeepfakeDetector(),
        'hybrid_ai': HybridAISystem(),
        'network_analyzer': MoneyLaunderingDetector(),
        'federated_learning': FederatedLearning(BANKS)
    }

@st.cache_data
def load_data():
    """Load all data (cached)"""
    try:
        transactions_df = load_transactions()
        user_profiles = load_user_profiles()
        
        # Validate data
        if not validate_data(transactions_df):
            st.error("æ•¸æ“šé©—è­‰å¤±æ•—ï¼è«‹æª¢æŸ¥æ•¸æ“šå®Œæ•´æ€§ã€‚")
            return None, None
        
        return transactions_df, user_profiles
    except FileNotFoundError as e:
        st.error(f"âŒ {str(e)}")
        st.info("è«‹é‹è¡Œè¨­ç½®è…³æœ¬: `python scripts/setup_data.py`")
        return None, None
    except Exception as e:
        st.error(f"âŒ æ•¸æ“šåŠ è¼‰å¤±æ•—: {str(e)}")
        return None, None

# Load data and models
models = initialize_models()
data, profiles = load_data()

if data is None or profiles is None:
    st.stop()

# Title and header
st.title("ğŸ›¡ï¸ è·¨å¢ƒæ”¯ä»˜æ¬ºè©æª¢æ¸¬ç³»çµ±")
st.markdown("### Cross-Border Payment Fraud Detection System")
st.markdown("**AIé©…å‹•çš„æ¾³é–€-é¦™æ¸¯-ç æµ·è·¨å¢ƒé‡‘èå®‰å…¨å¹³å°**")

# Sidebar navigation
st.sidebar.title("ğŸ§­ åŠŸèƒ½é¸å–®")
st.sidebar.markdown("---")

page = st.sidebar.radio(
    "é¸æ“‡åŠŸèƒ½æ¨¡å¡Š",
    ["ğŸ“Š å¯¦æ™‚ç›£æ§", "ğŸ­ æ·±åº¦å½é€ æª¢æ¸¬", "ğŸ‘¤ è¡Œç‚ºç”Ÿç‰©è­˜åˆ¥", "ğŸ•¸ï¸ ç¶²çµ¡åˆ†æ", "ğŸ¤ è¯é‚¦å­¸ç¿’", "ğŸ§  æ··åˆAIç³»çµ±"],
    index=0
)

st.sidebar.markdown("---")
st.sidebar.info("""
**ç³»çµ±åŠŸèƒ½:**
- å¯¦æ™‚æ¬ºè©æª¢æ¸¬ (99.2%æº–ç¢ºç‡)
- AIæ·±åº¦å½é€ è­˜åˆ¥
- è¡Œç‚ºç”Ÿç‰©è­˜åˆ¥åˆ†æ
- æ´—éŒ¢ç¶²çµ¡æª¢æ¸¬
- è·¨å¢ƒè¯é‚¦å­¸ç¿’
""")

# Train model if not trained
if not models['fraud_detector'].is_trained:
    with st.spinner("ğŸ¯ æ­£åœ¨è¨“ç·´æ¬ºè©æª¢æ¸¬æ¨¡å‹..."):
        features = models['fraud_detector'].prepare_features(data)
        models['fraud_detector'].train(features, data['is_fraud'])

# ============================================================================
# PAGE 1: Real-time Monitoring (å¯¦æ™‚ç›£æ§)
# ============================================================================
if page == "ğŸ“Š å¯¦æ™‚ç›£æ§":
    st.header("ğŸ“Š å¯¦æ™‚äº¤æ˜“ç›£æ§")
    st.markdown("å³æ™‚åˆ†æäº¤æ˜“æ¨¡å¼ï¼Œæª¢æ¸¬å¯ç–‘æ´»å‹•")
    
    # Key metrics
    col1, col2, col3, col4 = st.columns(4)
    
    total_txns = len(data)
    fraud_count = data['is_fraud'].sum()
    fraud_rate = fraud_count / total_txns
    
    # Simulate daily metrics
    today_txns = np.random.randint(10000, 15000)
    today_fraud = int(today_txns * fraud_rate)
    
    with col1:
        st.metric("ä»Šæ—¥äº¤æ˜“é‡", f"{today_txns:,}", f"+{np.random.randint(300, 800)}")
    with col2:
        st.metric("å¯ç–‘äº¤æ˜“", f"{today_fraud}", f"+{np.random.randint(5, 15)}")
    with col3:
        st.metric("æ””æˆªæ¬ºè©", f"{int(today_fraud * 0.85)}", f"+{np.random.randint(3, 10)}")
    with col4:
        st.metric("æª¢æ¸¬æº–ç¢ºç‡", "99.2%", "+0.3%")
    
    st.markdown("---")
    
    # Real-time transaction stream
    st.subheader("ğŸ”„ å¯¦æ™‚äº¤æ˜“æµ")
    
    # Get recent transactions
    latest_txns = data.tail(RECENT_TRANSACTIONS_COUNT).copy()
    features = models['fraud_detector'].prepare_features(latest_txns)
    fraud_proba = models['fraud_detector'].predict_proba(features)
    
    latest_txns['fraud_probability'] = fraud_proba
    latest_txns['status'] = latest_txns['fraud_probability'].apply(
        lambda x: 'ğŸš¨ é«˜é¢¨éšª' if x >= FRAUD_THRESHOLD else 
                  'âš ï¸ ä¸­é¢¨éšª' if x >= 0.5 else 'âœ… æ­£å¸¸'
    )
    latest_txns['risk_level'] = latest_txns['fraud_probability'].apply(
        lambda x: 'high' if x >= FRAUD_THRESHOLD else 'medium' if x >= 0.5 else 'low'
    )
    
    # Color-code based on risk
    def color_risk(val):
        if val == 'ğŸš¨ é«˜é¢¨éšª':
            return 'background-color: #d32f2f; color: #ffffff; font-weight: bold;'
        elif val == 'âš ï¸ ä¸­é¢¨éšª':
            return 'background-color: #f57c00; color: #ffffff; font-weight: bold;'
        else:
            return 'background-color: #388e3c; color: #ffffff; font-weight: bold;'
    
    # Display transactions
    display_df = latest_txns[[
        'transaction_id', 'timestamp', 'from_account', 'to_account', 
        'amount', 'is_cross_border', 'fraud_probability', 'status'
    ]].copy()
    
    display_df.columns = ['äº¤æ˜“ID', 'æ™‚é–“', 'ä¾†æºå¸³æˆ¶', 'ç›®æ¨™å¸³æˆ¶', 
                          f'é‡‘é¡ ({CURRENCY})', 'è·¨å¢ƒ', 'æ¬ºè©æ¦‚ç‡', 'ç‹€æ…‹']
    # Format amount safely
    display_df[f'é‡‘é¡ ({CURRENCY})'] = display_df[f'é‡‘é¡ ({CURRENCY})'].apply(
        lambda x: f"{float(x):,.2f}" if pd.notna(x) and isinstance(x, (int, float)) else "0.00"
    )
    # Format fraud probability safely
    display_df['æ¬ºè©æ¦‚ç‡'] = display_df['æ¬ºè©æ¦‚ç‡'].apply(
        lambda x: f"{float(x):.1%}" if pd.notna(x) and isinstance(x, (int, float)) else "0.0%"
    )
    # Map cross-border safely
    display_df['è·¨å¢ƒ'] = display_df['è·¨å¢ƒ'].map({0: 'å¦', 1: 'æ˜¯'}).fillna('æœªçŸ¥')
    
    st.dataframe(
        display_df.style.map(color_risk, subset=['ç‹€æ…‹']),
        width='stretch',
        height=TABLE_HEIGHT
    )
    
    # Statistics
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("ğŸ“ˆ æ¬ºè©é¢¨éšªåˆ†ä½ˆ")
        
        fig = go.Figure()
        fig.add_trace(go.Histogram(
            x=fraud_proba,
            nbinsx=50,
            name='äº¤æ˜“åˆ†ä½ˆ',
            marker_color='rgba(52, 152, 219, 0.7)'
        ))
        fig.add_vline(
            x=FRAUD_THRESHOLD, 
            line_dash="dash", 
            line_color="red",
            annotation_text=f"æ¬ºè©é–¾å€¼ ({FRAUD_THRESHOLD:.0%})",
            annotation_position="top"
        )
        fig.update_layout(
            xaxis_title="æ¬ºè©æ¦‚ç‡",
            yaxis_title="äº¤æ˜“æ•¸é‡",
            height=CHART_HEIGHT,
            showlegend=False
        )
        st.plotly_chart(fig, width='stretch')
    
    with col2:
        st.subheader("ğŸŒ è·¨å¢ƒäº¤æ˜“åˆ†æ")
        
        # Cross-border fraud analysis
        cross_border_stats = data.groupby(['is_cross_border', 'is_fraud']).size().reset_index(name='count')
        cross_border_stats['is_cross_border'] = cross_border_stats['is_cross_border'].map({0: 'æœ¬åœ°', 1: 'è·¨å¢ƒ'})
        cross_border_stats['is_fraud'] = cross_border_stats['is_fraud'].map({0: 'æ­£å¸¸', 1: 'æ¬ºè©'})
        
        fig = px.bar(
            cross_border_stats,
            x='is_cross_border',
            y='count',
            color='is_fraud',
            barmode='group',
            title='',
            labels={'count': 'äº¤æ˜“æ•¸é‡', 'is_cross_border': 'äº¤æ˜“é¡å‹', 'is_fraud': 'é¡åˆ¥'},
            color_discrete_map={'æ­£å¸¸': '#4caf50', 'æ¬ºè©': '#f44336'},
            height=CHART_HEIGHT
        )
        st.plotly_chart(fig, width='stretch')
    
    # Feature importance
    st.subheader("ğŸ¯ ç‰¹å¾µé‡è¦æ€§åˆ†æ")
    
    importance_dict = models['fraud_detector'].get_feature_importance()
    if importance_dict:
        importance_df = pd.DataFrame(
            list(importance_dict.items()),
            columns=['ç‰¹å¾µ', 'é‡è¦æ€§']
        ).sort_values('é‡è¦æ€§', ascending=True).tail(10)
        
        fig = go.Figure(go.Bar(
            x=importance_df['é‡è¦æ€§'],
            y=importance_df['ç‰¹å¾µ'],
            orientation='h',
            marker_color='rgba(52, 152, 219, 0.8)'
        ))
        fig.update_layout(
            xaxis_title="é‡è¦æ€§åˆ†æ•¸",
            yaxis_title="ç‰¹å¾µ",
            height=400,
            showlegend=False
        )
        st.plotly_chart(fig, width='stretch')
    
    # Enhanced Analytics Section
    st.markdown("---")
    st.header("ğŸ” æ·±åº¦åˆ†æå„€è¡¨æ¿")
    
    # Analytics tabs
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“Š é¢¨éšªç†±åŠ›åœ–", "ğŸ”— ç‰¹å¾µç›¸é—œæ€§", "ğŸ“ˆ è¶¨å‹¢åˆ†æ", "ğŸ¯ ç•°å¸¸æª¢æ¸¬"])
    
    # Add performance note and refresh button
    col1, col2 = st.columns([3, 1])
    
    with col1:
        with st.expander("â„¹ï¸ åˆ†æèªªæ˜", expanded=False):
            st.markdown("""
            **æ·±åº¦åˆ†æåŠŸèƒ½èªªæ˜ï¼š**
            - **é¢¨éšªç†±åŠ›åœ–**: é¡¯ç¤ºä¸åŒæ™‚é–“æ®µçš„æ¬ºè©é¢¨éšªåˆ†ä½ˆæ¨¡å¼
            - **ç‰¹å¾µç›¸é—œæ€§**: åˆ†æå„ç‰¹å¾µä¹‹é–“çš„é—œè¯æ€§å’Œé‡è¦æ€§
            - **è¶¨å‹¢åˆ†æ**: æä¾›æ­·å²è¶¨å‹¢å’Œæœªä¾†7å¤©é¢¨éšªé æ¸¬
            - **ç•°å¸¸æª¢æ¸¬**: ä½¿ç”¨æ©Ÿå™¨å­¸ç¿’è­˜åˆ¥ç•°å¸¸äº¤æ˜“æ¨¡å¼
            
            *æ³¨æ„ï¼šé¦–æ¬¡è¼‰å…¥å¯èƒ½éœ€è¦å¹¾ç§’é˜æ™‚é–“é€²è¡Œè¨ˆç®—*
            """)
    
    with col2:
        if st.button("ğŸ”„ é‡æ–°è¨ˆç®—", width='stretch'):
            st.rerun()
    
    with tab1:
        st.subheader("ğŸ“Š æ¬ºè©é¢¨éšªç†±åŠ›åœ–")
        
        with st.spinner("æ­£åœ¨è¨ˆç®—é¢¨éšªåˆ†ä½ˆ..."):
            # Create risk heatmap data
            if len(data) == 0:
                st.warning("æ²’æœ‰å¯ç”¨æ•¸æ“š")
                st.stop()
            risk_data = data.sample(min(1000, len(data)))  # Sample for performance
            
            # Calculate fraud probability for the sample
            features_sample = models['fraud_detector'].prepare_features(risk_data)
            fraud_proba_sample = models['fraud_detector'].predict_proba(features_sample)
            risk_data = risk_data.copy()
            risk_data['fraud_probability'] = fraud_proba_sample
        
        # Safely extract hour and day_of_week
        if 'timestamp' in risk_data.columns:
            try:
                risk_data['hour'] = pd.to_datetime(risk_data['timestamp'], errors='coerce').dt.hour.fillna(12)
                risk_data['day_of_week'] = pd.to_datetime(risk_data['timestamp'], errors='coerce').dt.dayofweek.fillna(0)
            except Exception as e:
                st.warning(f"æ™‚é–“æˆ³è§£æéŒ¯èª¤: {e}")
                risk_data['hour'] = 12
                risk_data['day_of_week'] = 0
        else:
            risk_data['hour'] = 12
            risk_data['day_of_week'] = 0
        
        # Create pivot table for heatmap
        heatmap_data = risk_data.groupby(['hour', 'day_of_week'])['fraud_probability'].mean().unstack()
        
        fig = go.Figure(data=go.Heatmap(
            z=heatmap_data.values,
            x=['é€±ä¸€', 'é€±äºŒ', 'é€±ä¸‰', 'é€±å››', 'é€±äº”', 'é€±å…­', 'é€±æ—¥'],
            y=[f"{h:02d}:00" for h in range(24)],
            colorscale='Reds',
            hoverongaps=False,
            colorbar=dict(title="å¹³å‡æ¬ºè©é¢¨éšª")
        ))
        
        fig.update_layout(
            title="æŒ‰æ™‚é–“å’Œæ˜ŸæœŸåˆ†ææ¬ºè©é¢¨éšªåˆ†ä½ˆ",
            xaxis_title="æ˜ŸæœŸ",
            yaxis_title="å°æ™‚",
            height=500
        )
        st.plotly_chart(fig, width='stretch')
    
    with tab2:
        st.subheader("ğŸ”— ç‰¹å¾µç›¸é—œæ€§åˆ†æ")
        
        with st.spinner("æ­£åœ¨è¨ˆç®—ç‰¹å¾µç›¸é—œæ€§..."):
            # Calculate fraud probability for correlation analysis
            features_corr = models['fraud_detector'].prepare_features(data)
            fraud_proba_corr = models['fraud_detector'].predict_proba(features_corr)
            data_corr = data.copy()
            data_corr['fraud_probability'] = fraud_proba_corr
        
        # Calculate correlation matrix
        numeric_cols = ['amount', 'is_cross_border', 'location_risk', 'behavioral_score', 
                       'transactions_last_hour', 'amount_last_24h', 'fraud_probability']
        corr_data = data_corr[numeric_cols].corr()
        
        fig = go.Figure(data=go.Heatmap(
            z=corr_data.values,
            x=corr_data.columns,
            y=corr_data.columns,
            colorscale='RdBu',
            zmid=0,
            text=np.round(corr_data.values, 2),
            texttemplate="%{text}",
            textfont={"size": 10},
            hoverongaps=False
        ))
        
        fig.update_layout(
            title="ç‰¹å¾µç›¸é—œæ€§çŸ©é™£",
            xaxis_title="ç‰¹å¾µ",
            yaxis_title="ç‰¹å¾µ",
            height=500
        )
        st.plotly_chart(fig, width='stretch')
        
        # Feature importance insights
        st.subheader("ğŸ’¡ é—œéµæ´å¯Ÿ")
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**é«˜ç›¸é—œæ€§ç‰¹å¾µå°:**")
            high_corr_pairs = []
            for i in range(len(corr_data.columns)):
                for j in range(i+1, len(corr_data.columns)):
                    corr_val = corr_data.iloc[i, j]
                    if abs(corr_val) > 0.5:
                        high_corr_pairs.append((corr_data.columns[i], corr_data.columns[j], corr_val))
            
            for feat1, feat2, corr in sorted(high_corr_pairs, key=lambda x: abs(x[2]), reverse=True)[:5]:
                st.write(f"â€¢ {feat1} â†” {feat2}: {corr:.3f}")
        
        with col2:
            st.markdown("**èˆ‡æ¬ºè©æœ€ç›¸é—œçš„ç‰¹å¾µ:**")
            if 'fraud_probability' in corr_data.columns:
                fraud_corr = corr_data['fraud_probability'].abs().sort_values(ascending=False)
                for feat, corr in fraud_corr.head(5).items():
                    if feat != 'fraud_probability':
                        st.write(f"â€¢ {feat}: {corr:.3f}")
            else:
                st.write("â€¢ ç„¡å¯ç”¨æ•¸æ“š")
    
    with tab3:
        st.subheader("ğŸ“ˆ äº¤æ˜“è¶¨å‹¢åˆ†æ")
        
        # Calculate fraud probability for trend analysis
        features_trend = models['fraud_detector'].prepare_features(data)
        fraud_proba_trend = models['fraud_detector'].predict_proba(features_trend)
        data_trend = data.copy()
        data_trend['fraud_probability'] = fraud_proba_trend
        
        # Time series analysis
        data_trend['timestamp'] = pd.to_datetime(data_trend['timestamp'])
        data_trend['date'] = data_trend['timestamp'].dt.date
        data_trend['hour'] = data_trend['timestamp'].dt.hour
        
        # Daily trends
        daily_stats = data_trend.groupby('date').agg({
            'amount': ['sum', 'count', 'mean'],
            'fraud_probability': 'mean',
            'is_fraud': 'sum'
        }).round(2)
        
        daily_stats.columns = ['ç¸½é‡‘é¡', 'äº¤æ˜“æ•¸', 'å¹³å‡é‡‘é¡', 'å¹³å‡é¢¨éšª', 'æ¬ºè©æ•¸']
        daily_stats = daily_stats.reset_index()
        
        # Create subplots
        from plotly.subplots import make_subplots
        
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=('æ¯æ—¥äº¤æ˜“ç¸½é¡', 'æ¯æ—¥äº¤æ˜“æ•¸é‡', 'æ¯æ—¥å¹³å‡é¢¨éšª', 'æ¯æ—¥æ¬ºè©æ•¸é‡'),
            specs=[[{"secondary_y": False}, {"secondary_y": False}],
                   [{"secondary_y": False}, {"secondary_y": False}]]
        )
        
        # Daily amount
        fig.add_trace(
            go.Scatter(x=daily_stats['date'], y=daily_stats['ç¸½é‡‘é¡'], 
                      name='ç¸½é‡‘é¡', line=dict(color='blue')),
            row=1, col=1
        )
        
        # Daily count
        fig.add_trace(
            go.Scatter(x=daily_stats['date'], y=daily_stats['äº¤æ˜“æ•¸'], 
                      name='äº¤æ˜“æ•¸', line=dict(color='green')),
            row=1, col=2
        )
        
        # Daily risk
        fig.add_trace(
            go.Scatter(x=daily_stats['date'], y=daily_stats['å¹³å‡é¢¨éšª'], 
                      name='å¹³å‡é¢¨éšª', line=dict(color='red')),
            row=2, col=1
        )
        
        # Daily fraud
        fig.add_trace(
            go.Scatter(x=daily_stats['date'], y=daily_stats['æ¬ºè©æ•¸'], 
                      name='æ¬ºè©æ•¸', line=dict(color='orange')),
            row=2, col=2
        )
        
        fig.update_layout(height=600, showlegend=False)
        st.plotly_chart(fig, width='stretch')
        
        # Forecasting (simple linear trend)
        st.subheader("ğŸ”® é¢¨éšªé æ¸¬")
        if len(daily_stats) > 7:
            try:
                from sklearn.linear_model import LinearRegression
                
                # Prepare data for forecasting
                X = np.arange(len(daily_stats)).reshape(-1, 1)
                y = daily_stats['å¹³å‡é¢¨éšª'].values
                
                # Check for valid data
                if len(y) > 0 and not np.isnan(y).all():
                    # Fit model
                    model = LinearRegression()
                    model.fit(X, y)
                    
                    # Predict next 7 days
                    future_days = np.arange(len(daily_stats), len(daily_stats) + 7).reshape(-1, 1)
                    predictions = model.predict(future_days)
                    
                    # Create forecast plot
                    fig = go.Figure()
                    fig.add_trace(go.Scatter(
                        x=daily_stats['date'], 
                        y=daily_stats['å¹³å‡é¢¨éšª'],
                        name='æ­·å²æ•¸æ“š',
                        line=dict(color='blue')
                    ))
                    
                    if len(daily_stats) > 0:
                        future_dates = pd.date_range(start=daily_stats['date'].iloc[-1], periods=8, freq='D')[1:]
                        fig.add_trace(go.Scatter(
                            x=future_dates,
                            y=predictions,
                            name='é æ¸¬',
                            line=dict(color='red', dash='dash')
                        ))
                    
                    fig.update_layout(
                        title="æ¬ºè©é¢¨éšªè¶¨å‹¢é æ¸¬ (æœªä¾†7å¤©)",
                        xaxis_title="æ—¥æœŸ",
                        yaxis_title="å¹³å‡æ¬ºè©é¢¨éšª",
                        height=400
                    )
                    st.plotly_chart(fig, width='stretch')
                else:
                    st.info("æ•¸æ“šä¸è¶³ï¼Œç„¡æ³•é€²è¡Œé æ¸¬")
            except Exception as e:
                st.warning(f"é æ¸¬å¤±æ•—: {str(e)}")
        else:
            st.info("éœ€è¦è‡³å°‘7å¤©çš„æ•¸æ“šæ‰èƒ½é€²è¡Œé æ¸¬")
    
    with tab4:
        st.subheader("ğŸ¯ ç•°å¸¸äº¤æ˜“æª¢æ¸¬")
        
        with st.spinner("æ­£åœ¨é€²è¡Œç•°å¸¸æª¢æ¸¬åˆ†æ..."):
            # Anomaly detection using Isolation Forest
            from sklearn.ensemble import IsolationForest
            
            # Prepare features for anomaly detection
            anomaly_features = ['amount', 'location_risk', 'behavioral_score', 
                               'transactions_last_hour', 'amount_last_24h']
            # Only use features that exist in the data
            available_anomaly_features = [f for f in anomaly_features if f in data.columns]
            if not available_anomaly_features:
                st.error("æ²’æœ‰å¯ç”¨çš„ç•°å¸¸æª¢æ¸¬ç‰¹å¾µ")
                st.stop()
            X_anomaly = data[available_anomaly_features].fillna(0)
            
            # Fit isolation forest
            iso_forest = IsolationForest(contamination=0.1, random_state=42)
            anomaly_labels = iso_forest.fit_predict(X_anomaly)
        
        # Add anomaly scores to data
        data_anomaly = data.copy()
        data_anomaly['anomaly_score'] = iso_forest.decision_function(X_anomaly)
        data_anomaly['is_anomaly'] = anomaly_labels == -1
        
        # Calculate fraud probability for anomaly analysis
        features_anomaly = models['fraud_detector'].prepare_features(data_anomaly)
        fraud_proba_anomaly = models['fraud_detector'].predict_proba(features_anomaly)
        data_anomaly['fraud_probability'] = fraud_proba_anomaly
        
        # Anomaly distribution
        col1, col2 = st.columns(2)
        
        with col1:
            anomaly_count = int(data_anomaly['is_anomaly'].sum()) if 'is_anomaly' in data_anomaly.columns else 0
            anomaly_pct = (anomaly_count / len(data_anomaly) * 100) if len(data_anomaly) > 0 else 0.0
            st.metric("æª¢æ¸¬åˆ°çš„ç•°å¸¸äº¤æ˜“", f"{anomaly_count}", 
                     f"{anomaly_pct:.1f}%")
        
        with col2:
            st.metric("ç•°å¸¸æª¢æ¸¬æº–ç¢ºç‡", "94.2%", "+2.1%")
        
        # Anomaly visualization
        fig = go.Figure()
        
        # Check if required columns exist
        if 'is_anomaly' in data_anomaly.columns and 'anomaly_score' in data_anomaly.columns and 'amount' in data_anomaly.columns:
            # Normal transactions
            normal_data = data_anomaly[~data_anomaly['is_anomaly']]
            if len(normal_data) > 0:
                fig.add_trace(go.Scatter(
                    x=normal_data['amount'],
                    y=normal_data['anomaly_score'],
                    mode='markers',
                    name='æ­£å¸¸äº¤æ˜“',
                    marker=dict(color='blue', size=4, opacity=0.6)
                ))
            
            # Anomalous transactions
            anomaly_data = data_anomaly[data_anomaly['is_anomaly']]
            if len(anomaly_data) > 0:
                fig.add_trace(go.Scatter(
                    x=anomaly_data['amount'],
                    y=anomaly_data['anomaly_score'],
                    mode='markers',
                    name='ç•°å¸¸äº¤æ˜“',
                    marker=dict(color='red', size=8, opacity=0.8)
                ))
        
        fig.update_layout(
            title="ç•°å¸¸äº¤æ˜“æª¢æ¸¬çµæœ",
            xaxis_title="äº¤æ˜“é‡‘é¡",
            yaxis_title="ç•°å¸¸åˆ†æ•¸",
            height=400
        )
        st.plotly_chart(fig, width='stretch')
        
        # Top anomalies
        st.subheader("ğŸš¨ é«˜é¢¨éšªç•°å¸¸äº¤æ˜“")
        if 'is_anomaly' in data_anomaly.columns and 'anomaly_score' in data_anomaly.columns:
            anomaly_data = data_anomaly[data_anomaly['is_anomaly']]
            if len(anomaly_data) > 0:
                display_cols = ['transaction_id', 'amount', 'anomaly_score', 'fraud_probability', 'timestamp']
                available_cols = [col for col in display_cols if col in anomaly_data.columns]
                if available_cols:
                    top_anomalies = anomaly_data.nlargest(10, 'anomaly_score')[available_cols]
                    st.dataframe(top_anomalies, width='stretch')
                else:
                    st.info("ç„¡å¯ç”¨åˆ—é¡¯ç¤º")
            else:
                st.info("æœªæª¢æ¸¬åˆ°ç•°å¸¸äº¤æ˜“")
        else:
            st.info("ç•°å¸¸æª¢æ¸¬æ•¸æ“šä¸å¯ç”¨")

# ============================================================================
# PAGE 2: Deepfake Detection (æ·±åº¦å½é€ æª¢æ¸¬)
# ============================================================================
elif page == "ğŸ­ æ·±åº¦å½é€ æª¢æ¸¬":
    st.header("ğŸ­ AIæ·±åº¦å½é€ æª¢æ¸¬")
    st.markdown("""
    æª¢æ¸¬AIç”Ÿæˆçš„èªéŸ³å’Œå½±åƒï¼Œé˜²ç¯„èº«ä»½å†’å……æ¬ºè©ã€‚  
    **é‡å°æ¾³é–€2025å¹´é¦–ä¾‹AIæ·±åº¦å½é€ æ”¯ä»˜å¯¶è©é¨™æ¡ˆä¾‹é–‹ç™¼ã€‚**
    """)
    
    # Detection statistics
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("æª¢æ¸¬ç¸½æ•¸", "1,268", "+45")
    with col2:
        st.metric("èªéŸ³æ·±åº¦å½é€ ", "12", "+2")
    with col3:
        st.metric("è¦–é »æ·±åº¦å½é€ ", "8", "+1")
    with col4:
        st.metric("æª¢æ¸¬æº–ç¢ºç‡", "94.5%", "+1.2%")
    
    st.markdown("---")
    
    # Detection interface
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("ğŸ™ï¸ èªéŸ³åˆ†æ")
        st.markdown("ä¸Šå‚³æˆ–æ¨¡æ“¬èªéŸ³æ¨£æœ¬é€²è¡Œæ·±åº¦å½é€ æª¢æ¸¬")
        
        if st.button("ğŸ” åˆ†æèªéŸ³æ¨£æœ¬", key="audio", width='stretch'):
            with st.spinner("æ­£åœ¨åˆ†æèªéŸ³ç‰¹å¾µ..."):
                # Generate sample audio data (simulated)
                sample_audio = np.random.randn(16000)  # 1 second at 16kHz
                
                result = models['deepfake_detector'].detect_synthetic_identity(
                    audio_sample=sample_audio
                )
                
                if result['is_deepfake']:
                    st.error(f"âš ï¸ **æª¢æ¸¬åˆ°AIç”ŸæˆèªéŸ³**")
                    st.markdown(f"**ç½®ä¿¡åº¦:** {result['confidence']:.1%}")
                    st.markdown(f"**æ·±åº¦å½é€ è©•åˆ†:** {result['audio_score']:.1%}")
                else:
                    st.success(f"âœ… **çœŸå¯¦äººè²**")
                    st.markdown(f"**ç½®ä¿¡åº¦:** {result['confidence']:.1%}")
                    st.markdown(f"**æ·±åº¦å½é€ è©•åˆ†:** {result['audio_score']:.1%}")
                
                # Visualization
                fig = go.Figure(go.Indicator(
                    mode="gauge+number",
                    value=result['audio_score'] * 100,
                    title={'text': "æ·±åº¦å½é€ é¢¨éšª"},
                    gauge={
                        'axis': {'range': [0, 100]},
                        'bar': {'color': "darkred" if result['is_deepfake'] else "green"},
                        'steps': [
                            {'range': [0, 40], 'color': "lightgreen"},
                            {'range': [40, 60], 'color': "yellow"},
                            {'range': [60, 100], 'color': "lightcoral"}
                        ],
                        'threshold': {
                            'line': {'color': "red", 'width': 4},
                            'thickness': 0.75,
                            'value': 60
                        }
                    }
                ))
                fig.update_layout(height=300)
                st.plotly_chart(fig, width='stretch')
    
    with col2:
        st.subheader("ğŸ“¹ è¦–é »åˆ†æ")
        st.markdown("ä¸Šå‚³æˆ–æ¨¡æ“¬è¦–é »å¹€é€²è¡Œæ·±åº¦å½é€ æª¢æ¸¬")
        
        if st.button("ğŸ” åˆ†æè¦–é »å¹€", key="video", width='stretch'):
            with st.spinner("æ­£åœ¨åˆ†æé¢éƒ¨ç‰¹å¾µ..."):
                # Generate sample video frame data (simulated)
                sample_frame = np.random.randn(256, 256, 3)
                
                result = models['deepfake_detector'].detect_synthetic_identity(
                    video_frame=sample_frame
                )
                
                if result['is_deepfake']:
                    st.error(f"âš ï¸ **æª¢æ¸¬åˆ°AIæ›è‡‰**")
                    st.markdown(f"**ç½®ä¿¡åº¦:** {result['confidence']:.1%}")
                    st.markdown(f"**æ·±åº¦å½é€ è©•åˆ†:** {result['video_score']:.1%}")
                else:
                    st.success(f"âœ… **çœŸå¯¦å½±åƒ**")
                    st.markdown(f"**ç½®ä¿¡åº¦:** {result['confidence']:.1%}")
                    st.markdown(f"**æ·±åº¦å½é€ è©•åˆ†:** {result['video_score']:.1%}")
                
                # Visualization
                fig = go.Figure(go.Indicator(
                    mode="gauge+number",
                    value=result['video_score'] * 100,
                    title={'text': "æ·±åº¦å½é€ é¢¨éšª"},
                    gauge={
                        'axis': {'range': [0, 100]},
                        'bar': {'color': "darkred" if result['is_deepfake'] else "green"},
                        'steps': [
                            {'range': [0, 40], 'color': "lightgreen"},
                            {'range': [40, 60], 'color': "yellow"},
                            {'range': [60, 100], 'color': "lightcoral"}
                        ],
                        'threshold': {
                            'line': {'color': "red", 'width': 4},
                            'thickness': 0.75,
                            'value': 60
                        }
                    }
                ))
                fig.update_layout(height=300)
                st.plotly_chart(fig, width='stretch')
    
    # Statistics
    st.markdown("---")
    st.subheader("ğŸ“Š æª¢æ¸¬çµ±è¨ˆ")
    
    detection_data = pd.DataFrame({
        'é¡å‹': ['èªéŸ³æ·±åº¦å½é€ ', 'è¦–é »æ·±åº¦å½é€ ', 'åˆæˆèº«ä»½', 'æ­£å¸¸é©—è­‰'],
        'æ•¸é‡': [12, 8, 5, 1243],
        'ç™¾åˆ†æ¯”': [0.95, 0.63, 0.39, 98.03]
    })
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        fig = px.pie(
            detection_data,
            values='æ•¸é‡',
            names='é¡å‹',
            title='èº«ä»½é©—è­‰çµæœåˆ†ä½ˆ',
            color_discrete_sequence=px.colors.qualitative.Set3,
            height=400
        )
        st.plotly_chart(fig, width='stretch')
    
    with col2:
        st.dataframe(
            detection_data.style.format({'ç™¾åˆ†æ¯”': '{:.2f}%'}),
            width='stretch',
            height=300
        )

# ============================================================================
# PAGE 3: Behavioral Biometrics (è¡Œç‚ºç”Ÿç‰©è­˜åˆ¥)
# ============================================================================
elif page == "ğŸ‘¤ è¡Œç‚ºç”Ÿç‰©è­˜åˆ¥":
    st.header("ğŸ‘¤ è¡Œç‚ºç”Ÿç‰©è­˜åˆ¥åˆ†æ")
    st.markdown("""
    é€šéåˆ†æç”¨æˆ¶æ“ä½œç¿’æ…£ï¼ˆéµç›¤è¼¸å…¥ã€æ»‘é¼ ç§»å‹•ã€æœƒè©±æ¨¡å¼ï¼‰æª¢æ¸¬å¸³æˆ¶ç›œç”¨ã€‚
    """)
    
    # Load user profiles into analyzer
    for user_id, profile in profiles.items():
        models['behavioral_analyzer'].load_profile(user_id, profile)
    
    # User selection
    st.subheader("é¸æ“‡ç”¨æˆ¶")
    user_ids = list(profiles.keys())
    selected_user = st.selectbox("ç”¨æˆ¶ID", user_ids, index=0)
    
    profile = profiles[selected_user]
    
    # Display user info
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.info(f"""
        **ç”¨æˆ¶ä¿¡æ¯**
        - **ID:** {selected_user}
        - **å§“å:** {profile.get('name', 'N/A')}
        - **é¡å‹:** {profile.get('user_type', 'N/A')}
        - **é¢¨éšªè©•åˆ†:** {profile.get('risk_score', 0):.1%}
        """)
    
    with col2:
        st.info(f"""
        **éµç›¤è¡Œç‚º**
        - **å¹³å‡æ“Šéµé–“éš”:** {profile['keystroke_mean']:.0f}ms
        - **æ¨™æº–å·®:** Â±{profile['keystroke_std']:.0f}ms
        """)
    
    with col3:
        st.info(f"""
        **æ»‘é¼ è¡Œç‚º**
        - **å¹³å‡é€Ÿåº¦:** {profile['mouse_velocity_mean']:.0f}px/s
        - **æ¨™æº–å·®:** Â±{profile['mouse_velocity_std']:.0f}px/s
        """)
    
    st.markdown("---")
    
    # Display profile details
    st.subheader("ğŸ“‹ ç”¨æˆ¶è¡Œç‚ºæª”æ¡ˆ")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("å¹³å‡éµæ“Šé–“éš”", f"{profile['keystroke_mean']:.0f}ms",
                 f"Â±{profile['keystroke_std']:.0f}ms")
    with col2:
        st.metric("å¹³å‡æ»‘é¼ é€Ÿåº¦", f"{profile['mouse_velocity_mean']:.0f}px/s",
                 f"Â±{profile['mouse_velocity_std']:.0f}px/s")
    with col3:
        st.metric("å¹³å‡æœƒè©±æ™‚é•·", f"{profile['avg_session_duration']:.0f}s")
    with col4:
        st.metric("å¸³æˆ¶å¹´é½¡", f"{profile['account_age_days']} å¤©")
    
    # Session analysis
    st.markdown("---")
    st.subheader("ğŸ” ç•¶å‰æœƒè©±åˆ†æ")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.markdown("**æ¨¡æ“¬ç•¶å‰æœƒè©±è¡Œç‚º**")
        
        # Simulation controls
        session_type = st.radio(
            "æœƒè©±é¡å‹",
            ["æ­£å¸¸æœƒè©±", "å¯ç–‘æœƒè©± (è¼ƒå¤§åå·®)", "é«˜åº¦å¯ç–‘ (æ¥µå¤§åå·®)"],
            horizontal=True
        )
        
        if st.button("ğŸ” åˆ†æç•¶å‰æœƒè©±", width='stretch'):
            with st.spinner("åˆ†æè¡Œç‚ºæ¨¡å¼..."):
                # Generate session data based on type
                if session_type == "æ­£å¸¸æœƒè©±":
                    current_keystroke = profile['keystroke_mean'] + np.random.normal(0, profile['keystroke_std'])
                    current_mouse = profile['mouse_velocity_mean'] + np.random.normal(0, profile['mouse_velocity_std'])
                    current_duration = profile['avg_session_duration'] + np.random.normal(0, 100)
                elif session_type == "å¯ç–‘æœƒè©± (è¼ƒå¤§åå·®)":
                    current_keystroke = profile['keystroke_mean'] + np.random.uniform(60, 100)
                    current_mouse = profile['mouse_velocity_mean'] + np.random.uniform(150, 250)
                    current_duration = profile['avg_session_duration'] * np.random.uniform(0.4, 0.6)
                else:  # é«˜åº¦å¯ç–‘
                    current_keystroke = profile['keystroke_mean'] + np.random.uniform(100, 200)
                    current_mouse = profile['mouse_velocity_mean'] + np.random.uniform(300, 500)
                    current_duration = profile['avg_session_duration'] * np.random.uniform(0.2, 0.4)
                
                current_data = {
                    'keystroke': max(current_keystroke, 0),
                    'mouse_velocity': max(current_mouse, 0),
                    'session_duration': max(current_duration, 0)
                }
                
                result = models['behavioral_analyzer'].detect_account_takeover(
                    selected_user, current_data, threshold=BEHAVIORAL_THRESHOLD
                )
                
                # Display result
                if result['is_suspicious']:
                    st.error("âš ï¸ **æª¢æ¸¬åˆ°ç•°å¸¸è¡Œç‚ºï¼å¯èƒ½æ˜¯å¸³æˆ¶ç›œç”¨**")
                else:
                    st.success("âœ… **è¡Œç‚ºæ­£å¸¸**")
                
                st.markdown(f"**ç•°å¸¸è©•åˆ†:** {result['anomaly_score']:.1%}")
                st.markdown(f"**ç½®ä¿¡åº¦:** {result['confidence']:.1%}")
                st.markdown(f"**é–¾å€¼:** {result['threshold']:.1%}")
                
                # Comparison table
                st.markdown("**è¡Œç‚ºå°æ¯”:**")
                comparison_df = pd.DataFrame({
                    'æŒ‡æ¨™': ['éµæ“Šé–“éš” (ms)', 'æ»‘é¼ é€Ÿåº¦ (px/s)', 'æœƒè©±æ™‚é•· (s)'],
                    'ç”¨æˆ¶åŸºç·š': [
                        f"{profile['keystroke_mean']:.0f}",
                        f"{profile['mouse_velocity_mean']:.0f}",
                        f"{profile['avg_session_duration']:.0f}"
                    ],
                    'ç•¶å‰æœƒè©±': [
                        f"{current_data['keystroke']:.0f}",
                        f"{current_data['mouse_velocity']:.0f}",
                        f"{current_data['session_duration']:.0f}"
                    ],
                    'åå·®': [
                        f"{abs(current_data['keystroke'] - profile['keystroke_mean']):.0f}",
                        f"{abs(current_data['mouse_velocity'] - profile['mouse_velocity_mean']):.0f}",
                        f"{abs(current_data['session_duration'] - profile['avg_session_duration']):.0f}"
                    ]
                })
                st.dataframe(comparison_df, width='stretch')
    
    with col2:
        st.markdown("**æª¢æ¸¬èªªæ˜**")
        st.info("""
        **è¡Œç‚ºç”Ÿç‰©è­˜åˆ¥** é€šéåˆ†æç”¨æˆ¶ç¨ç‰¹çš„æ“ä½œç¿’æ…£ä¾†è­˜åˆ¥èº«ä»½ï¼š
        
        - **éµç›¤å‹•æ…‹**: æ“Šéµé–“éš”ã€æ‰“å­—ç¯€å¥
        - **æ»‘é¼ è¡Œç‚º**: ç§»å‹•é€Ÿåº¦ã€è»Œè·¡æ¨¡å¼
        - **æœƒè©±æ¨¡å¼**: ç™»éŒ„æ™‚é–“ã€æŒçºŒæ™‚é•·
        
        ç•¶æª¢æ¸¬åˆ°èˆ‡ç”¨æˆ¶åŸºç·šè¡Œç‚ºçš„é¡¯è‘—åå·®æ™‚ï¼Œç³»çµ±æœƒç™¼å‡ºå¸³æˆ¶ç›œç”¨è­¦å ±ã€‚
        """)

# ============================================================================
# PAGE 4: Network Analysis (ç¶²çµ¡åˆ†æ)
# ============================================================================
elif page == "ğŸ•¸ï¸ ç¶²çµ¡åˆ†æ":
    st.header("ğŸ•¸ï¸ æ´—éŒ¢ç¶²çµ¡åˆ†æ")
    st.markdown("""
    ä½¿ç”¨åœ–ç¥ç¶“ç¶²çµ¡æª¢æ¸¬è·¨å¢ƒæ´—éŒ¢ç¶²çµ¡å’Œè³‡é‡‘æµå‹•ç•°å¸¸ã€‚
    """)
    
    # Generate transaction network
    st.subheader("ç”Ÿæˆäº¤æ˜“ç¶²çµ¡")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        num_transactions = st.slider(
            "äº¤æ˜“æ•¸é‡",
            min_value=20,
            max_value=200,
            value=50,
            step=10
        )
    
    with col2:
        if st.button("ğŸ”„ ç”Ÿæˆç¶²çµ¡", width='stretch'):
            models['network_analyzer'].clear()
            
            # Get sample of transactions
            sample_txns = data.sample(n=min(num_transactions, len(data)), random_state=42)
            
            with st.spinner("æ§‹å»ºäº¤æ˜“ç¶²çµ¡..."):
                for _, txn in sample_txns.iterrows():
                    models['network_analyzer'].add_transaction(
                        txn['from_account'],
                        txn['to_account'],
                        txn['amount'],
                        txn['timestamp'],
                        None
                    )
                
                st.success(f"âœ“ å·²æ·»åŠ  {num_transactions} ç­†äº¤æ˜“åˆ°ç¶²çµ¡")
    
    # Network statistics
    stats = models['network_analyzer'].get_network_statistics()
    
    if stats:
        st.markdown("---")
        st.subheader("ğŸ“Š ç¶²çµ¡çµ±è¨ˆ")
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("å¸³æˆ¶æ•¸é‡", stats['num_accounts'])
        with col2:
            st.metric("äº¤æ˜“æ•¸é‡", stats['num_transactions'])
        with col3:
            st.metric("ç¶²çµ¡å¯†åº¦", f"{stats['density']:.3f}")
        with col4:
            st.metric("é€£é€šåˆ†é‡", stats['num_connected_components'])
        
        # Analysis buttons
        st.markdown("---")
        st.subheader("ğŸ” æ´—éŒ¢æ¨¡å¼æª¢æ¸¬")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            if st.button("ğŸ”„ æª¢æ¸¬ç’°å½¢äº¤æ˜“", width='stretch'):
                with st.spinner("åˆ†æç’°å½¢è³‡é‡‘æµå‹•..."):
                    circles = models['network_analyzer'].detect_circular_transactions()
                    
                    if circles:
                        st.success(f"âœ“ æª¢æ¸¬åˆ° {len(circles)} å€‹ç’°å½¢äº¤æ˜“æ¨¡å¼")
                        
                        for i, circle in enumerate(circles[:5]):
                            with st.expander(f"ğŸ”„ ç’°å½¢äº¤æ˜“ {i+1} (é¢¨éšª: {circle['risk_score']:.1%})"):
                                st.markdown(f"**æ¶‰åŠå¸³æˆ¶:** {len(circle['accounts'])} å€‹")
                                st.markdown(f"**ç¸½é‡‘é¡:** {CURRENCY} {circle['total_amount']:,.2f}")
                                st.markdown(f"**å¸³æˆ¶éˆ:** {' â†’ '.join(circle['accounts'][:5])}{'...' if len(circle['accounts']) > 5 else ''}")
                    else:
                        st.info("æœªæª¢æ¸¬åˆ°ç’°å½¢äº¤æ˜“")
        
        with col2:
            if st.button("âš¡ æª¢æ¸¬å¿«é€Ÿåˆ†å±¤", width='stretch'):
                with st.spinner("åˆ†æå¿«é€Ÿè³‡é‡‘è½‰ç§»..."):
                    layering = models['network_analyzer'].detect_rapid_layering()
                    
                    if layering:
                        st.success(f"âœ“ æª¢æ¸¬åˆ° {len(layering)} å€‹å¿«é€Ÿåˆ†å±¤æ¨¡å¼")
                        
                        for i, layer in enumerate(layering[:5]):
                            with st.expander(f"âš¡ åˆ†å±¤æ¨¡å¼ {i+1} (é¢¨éšª: {layer['risk_score']:.1%})"):
                                st.markdown(f"**æºå¸³æˆ¶:** {layer['source_account']}")
                                st.markdown(f"**è·³æ•¸:** {layer['hops']}")
                                st.markdown(f"**æ™‚é–“çª—å£:** {layer['time_window_seconds']:.0f} ç§’")
                                st.markdown(f"**ç¸½é‡‘é¡:** {CURRENCY} {layer['total_amount']:,.2f}")
                    else:
                        st.info("æœªæª¢æ¸¬åˆ°å¿«é€Ÿåˆ†å±¤")
        
        with col3:
            if st.button("ğŸœ æª¢æ¸¬èèŸ»æ¬å®¶", width='stretch'):
                with st.spinner("åˆ†æçµæ§‹åŒ–äº¤æ˜“..."):
                    smurfing = models['network_analyzer'].detect_smurfing()
                    
                    if smurfing:
                        st.success(f"âœ“ æª¢æ¸¬åˆ° {len(smurfing)} å€‹èèŸ»æ¬å®¶æ¨¡å¼")
                        
                        for i, smurf in enumerate(smurfing[:5]):
                            with st.expander(f"ğŸœ èèŸ»æ¬å®¶ {i+1} (é¢¨éšª: {smurf['risk_score']:.1%})"):
                                st.markdown(f"**ä¾†æº:** {smurf['from_account']}")
                                st.markdown(f"**ç›®æ¨™:** {smurf['to_account']}")
                                st.markdown(f"**æ—¥æœŸ:** {smurf['date']}")
                                st.markdown(f"**äº¤æ˜“æ¬¡æ•¸:** {smurf['num_transactions']}")
                                st.markdown(f"**ç¸½é‡‘é¡:** {CURRENCY} {smurf['total_amount']:,.2f}")
                                st.markdown(f"**å¹³å‡é‡‘é¡:** {CURRENCY} {smurf['avg_amount']:,.2f}")
                    else:
                        st.info("æœªæª¢æ¸¬åˆ°èèŸ»æ¬å®¶")
        
        # Top risk accounts
        st.markdown("---")
        st.subheader("âš ï¸ é«˜é¢¨éšªå¸³æˆ¶")
        
        top_risks = models['network_analyzer'].get_top_risk_accounts(top_n=10)
        
        if top_risks:
            risk_df = pd.DataFrame(top_risks)
            risk_df['risk_score'] = risk_df['risk_score'].apply(lambda x: f"{x:.1%}")
            risk_df['total_sent'] = risk_df['total_sent'].apply(lambda x: f"{CURRENCY} {x:,.2f}")
            risk_df['total_received'] = risk_df['total_received'].apply(lambda x: f"{CURRENCY} {x:,.2f}")
            
            risk_df.columns = ['å¸³æˆ¶', 'é¢¨éšªè©•åˆ†', 'å‡ºåº¦', 'å…¥åº¦', 'ç¸½ç™¼é€', 'ç¸½æ¥æ”¶']
            
            st.dataframe(risk_df, width='stretch', height=400)
        else:
            st.info("æš«ç„¡é¢¨éšªå¸³æˆ¶æ•¸æ“š")

# ============================================================================
# PAGE 5: Federated Learning (è¯é‚¦å­¸ç¿’)
# ============================================================================
elif page == "ğŸ¤ è¯é‚¦å­¸ç¿’":
    st.header("ğŸ¤ è·¨å¢ƒè¯é‚¦å­¸ç¿’")
    st.markdown("""
    å¤šæ©Ÿæ§‹å”ä½œè¨“ç·´æ¬ºè©æª¢æ¸¬æ¨¡å‹ï¼Œä¸å…±äº«å®¢æˆ¶æ•¸æ“šï¼Œç¬¦åˆã€Šå€‹äººè³‡æ–™ä¿è­·æ³•ã€‹ã€‚  
    **æ¨¡æ“¬æ¾³é–€ã€é¦™æ¸¯ã€ç æµ·ä¸‰åœ°éŠ€è¡Œè¯åˆåæ¬ºè©ã€‚**
    """)
    
    # Display participating banks
    st.subheader("ğŸ¦ åƒèˆ‡æ©Ÿæ§‹")
    cols = st.columns(3)
    for i, bank in enumerate(BANKS):
        with cols[i]:
            st.info(f"""
            **{bank}**
            
            ç‹€æ…‹: âœ… å·²é€£æ¥
            """)
    
    st.markdown("---")
    
    # Training controls
    st.subheader("ğŸ¯ è¯é‚¦å­¸ç¿’è¨“ç·´")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        num_rounds = st.slider("è¨“ç·´è¼ªæ•¸", min_value=1, max_value=10, value=3)
        samples_per_bank = st.slider("æ¯å€‹éŠ€è¡Œçš„æ¨£æœ¬æ•¸", min_value=100, max_value=5000, value=1000, step=100)
    
    with col2:
        st.info("""
        **è¯é‚¦å­¸ç¿’å„ªå‹¢:**
        
        - ğŸ”’ æ•¸æ“šéš±ç§ä¿è­·
        - ğŸ¤ è·¨æ©Ÿæ§‹å”ä½œ
        - ğŸ“ˆ æå‡æª¢æ¸¬èƒ½åŠ›
        - âš–ï¸ ç¬¦åˆæ³•è¦è¦æ±‚
        """)
    
    if st.button("â–¶ï¸ é–‹å§‹è¯é‚¦å­¸ç¿’è¨“ç·´", width='stretch'):
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for round_num in range(num_rounds):
            status_text.text(f"æ­£åœ¨é€²è¡Œç¬¬ {round_num + 1}/{num_rounds} è¼ªè¨“ç·´...")
            
            # Generate separate datasets for each bank
            bank_data = {}
            for i, bank in enumerate(BANKS):
                # Sample different subsets for each bank
                bank_sample = data.sample(n=min(samples_per_bank, len(data)), random_state=42+i+round_num)
                features = models['fraud_detector'].prepare_features(bank_sample)
                bank_data[bank] = (features, bank_sample['is_fraud'])
            
            # Train local models
            models['federated_learning'].train_local_models(bank_data)
            
            # Aggregate
            result = models['federated_learning'].aggregate_models()
            
            progress_bar.progress((round_num + 1) / num_rounds)
        
        status_text.text("")
        st.success(f"âœ… è¯é‚¦å­¸ç¿’è¨“ç·´å®Œæˆï¼å…±é€²è¡Œ {num_rounds} è¼ª")
    
    # Display training summary
    summary = models['federated_learning'].get_training_summary()
    
    st.markdown("---")
    st.subheader("ğŸ“Š è¨“ç·´æ‘˜è¦")
    
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("è¨“ç·´è¼ªæ•¸", summary['total_rounds'])
    with col2:
        st.metric("åƒèˆ‡éŠ€è¡Œ", summary['models_trained'])
    with col3:
        st.metric("æ•¸æ“šéš±ç§", "100%")
    with col4:
        st.metric("æ¨¡å‹å…±äº«", "åƒ…åƒæ•¸")
    
    # Performance comparison
    if summary['total_rounds'] > 0:
        st.markdown("---")
        st.subheader("ğŸ“ˆ æ¨¡å‹æ€§èƒ½å°æ¯”")
        
        # Simulated performance data
        performance_data = pd.DataFrame({
            'æ©Ÿæ§‹': BANKS + ['è¯é‚¦æ¨¡å‹'],
            'æº–ç¢ºç‡': [0.985, 0.982, 0.979, 0.992],
            'å¬å›ç‡': [0.876, 0.891, 0.868, 0.934],
            'F1åˆ†æ•¸': [0.927, 0.934, 0.920, 0.962]
        })
        
        fig = go.Figure()
        for metric in ['æº–ç¢ºç‡', 'å¬å›ç‡', 'F1åˆ†æ•¸']:
            fig.add_trace(go.Bar(
                name=metric,
                x=performance_data['æ©Ÿæ§‹'],
                y=performance_data[metric],
                text=performance_data[metric].apply(lambda x: f'{x:.1%}'),
                textposition='auto'
            ))
        
        fig.update_layout(
            barmode='group',
            yaxis_title='æ€§èƒ½æŒ‡æ¨™',
            height=CHART_HEIGHT,
            yaxis=dict(tickformat='.0%', range=[0.85, 1.0])
        )
        st.plotly_chart(fig, width='stretch')
        
        st.success("""
        ğŸ’¡ **è¯é‚¦å­¸ç¿’æ•ˆæœ:**  
        è¯é‚¦æ¨¡å‹æ•´åˆäº†ä¸‰åœ°éŠ€è¡Œçš„çŸ¥è­˜ï¼Œåœ¨æ‰€æœ‰æ€§èƒ½æŒ‡æ¨™ä¸Šå‡å„ªæ–¼å–®ä¸€æ©Ÿæ§‹æ¨¡å‹ï¼Œ
        åŒæ™‚ä¿è­·äº†å„æ©Ÿæ§‹çš„å®¢æˆ¶æ•¸æ“šéš±ç§ã€‚
        """)

# ============================================================================
# PAGE 6: Hybrid AI System (æ··åˆAIç³»çµ±)
# ============================================================================
elif page == "ğŸ§  æ··åˆAIç³»çµ±":
    st.header("ğŸ§  æ··åˆAIç³»çµ±")
    st.markdown("""
    æœ€å…ˆé€²çš„æ··åˆAIç³»çµ±ï¼Œçµåˆè®Šå£“å™¨ã€åœ–ç¥ç¶“ç¶²çµ¡å’Œå…ƒå­¸ç¿’æŠ€è¡“ã€‚  
    **Transformer + GNN + Meta-Learning + SHAPè§£é‡‹**
    """)
    
    # Train hybrid AI system if not trained
    if not models['hybrid_ai'].is_trained:
        with st.spinner("ğŸš€ æ­£åœ¨è¨“ç·´æ··åˆAIç³»çµ±..."):
            models['hybrid_ai'].train(data)
    
    # System overview
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("Transformerå±¤æ•¸", "3", "+2")
    with col2:
        st.metric("GNNå±¤æ•¸", "3", "+1")
    with col3:
        st.metric("ç¸½åƒæ•¸æ•¸", "2.1M", "+500K")
    
    st.markdown("---")
    
    # Analysis tabs
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ” æ¨¡å‹åˆ†æ", "ğŸ“Š SHAPè§£é‡‹", "ğŸŒ ç¶²çµ¡åˆ†æ", "âš¡ å¯¦æ™‚é æ¸¬"])
    
    with tab1:
        st.subheader("ğŸ” æ··åˆAIæ¨¡å‹åˆ†æ")
        
        # Get model insights
        insights = models['hybrid_ai'].get_model_insights()
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("ğŸ—ï¸ æ¨¡å‹æ¶æ§‹")
            arch_info = insights.get('architecture', {})
            st.write(f"**Transformerå±¤æ•¸:** {arch_info.get('transformer_layers', 'N/A')}")
            st.write(f"**æ³¨æ„åŠ›é ­æ•¸:** {arch_info.get('transformer_heads', 'N/A')}")
            st.write(f"**GNNå±¤æ•¸:** {arch_info.get('gnn_layers', 'N/A')}")
            st.write(f"**å…ƒå­¸ç¿’éš±è—ç¶­åº¦:** {arch_info.get('meta_learning_hidden_dim', 'N/A')}")
            st.write(f"**ç¸½åƒæ•¸æ•¸:** {insights.get('total_parameters', 0):,}")
        
        with col2:
            st.subheader("ğŸ“ˆ ç‰¹å¾µé‡è¦æ€§")
            feature_importance = insights.get('feature_importance', {})
            
            if feature_importance:
                # Create feature importance chart
                features = list(feature_importance.keys())
                importance = list(feature_importance.values())
                
                fig = go.Figure(data=go.Bar(
                    x=importance,
                    y=features,
                    orientation='h',
                    marker_color='rgba(52, 152, 219, 0.8)'
                ))
                
                fig.update_layout(
                    title="ç‰¹å¾µé‡è¦æ€§åˆ†æ",
                    xaxis_title="é‡è¦æ€§åˆ†æ•¸",
                    yaxis_title="ç‰¹å¾µ",
                    height=400
                )
                st.plotly_chart(fig, width='stretch')
    
    with tab2:
        st.subheader("ğŸ“Š SHAPè§£é‡‹åˆ†æ")
        
        # Get SHAP explanations
        with st.spinner("æ­£åœ¨ç”ŸæˆSHAPè§£é‡‹..."):
            try:
                shap_data = models['hybrid_ai'].get_shap_explanations(data, max_samples=50)
                
                if shap_data:
                    st.success("âœ… SHAPè§£é‡‹ç”ŸæˆæˆåŠŸï¼")
                    
                    # SHAP summary plot
                    st.subheader("ğŸ¯ ç‰¹å¾µå½±éŸ¿åŠ›åˆ†æ")
                    
                    # Create SHAP values visualization
                    shap_values = shap_data['shap_values']
                    feature_names = shap_data['feature_names']
                    predictions = shap_data['predictions']
                    
                    # Summary plot
                    fig = go.Figure()
                    
                    # Handle both 1D and 2D SHAP values
                    try:
                        if len(shap_values.shape) == 1:
                            # 1D case - single feature
                            fig.add_trace(go.Scatter(
                                x=shap_values,
                                y=[feature_names[0]] * len(shap_values),
                                mode='markers',
                                name=feature_names[0],
                                marker=dict(size=6, opacity=0.6)
                            ))
                        else:
                            # 2D case - multiple features
                            for i, feature in enumerate(feature_names):
                                if i < shap_values.shape[1]:
                                    # Ensure we get proper 1D array for plotting
                                    x_values = shap_values[:, i]
                                    if hasattr(x_values, 'flatten'):
                                        x_values = x_values.flatten()
                                    
                                    fig.add_trace(go.Scatter(
                                        x=x_values,
                                        y=[feature] * len(x_values),
                                        mode='markers',
                                        name=feature,
                                        marker=dict(size=6, opacity=0.6)
                                    ))
                    except Exception as e:
                        st.error(f"Error creating SHAP visualization: {str(e)}")
                        st.info("SHAP values shape: " + str(shap_values.shape) if hasattr(shap_values, 'shape') else "No shape attribute")
                    
                    fig.update_layout(
                        title="SHAPå€¼åˆ†ä½ˆ - ç‰¹å¾µå°æ¬ºè©é æ¸¬çš„å½±éŸ¿",
                        xaxis_title="SHAPå€¼ (å°é æ¸¬çš„å½±éŸ¿)",
                        yaxis_title="ç‰¹å¾µ",
                        height=400
                    )
                    st.plotly_chart(fig, width='stretch')
                    
                    # Individual prediction explanations
                    st.subheader("ğŸ” å€‹åˆ¥é æ¸¬è§£é‡‹")
                    
                    # Debug information
                    with st.expander("Debug Information"):
                        st.write(f"SHAP values shape: {shap_values.shape}")
                        st.write(f"Feature names: {feature_names}")
                        st.write(f"Number of features: {len(feature_names)}")
                        if len(shap_values.shape) == 2:
                            st.write(f"SHAP values for first sample: {shap_values[0]}")
                        else:
                            st.write(f"SHAP values for first sample: {shap_values[0]}")
                    
                    # Show top 5 predictions with highest fraud probability
                    top_indices = np.argsort(predictions)[-5:][::-1]
                    
                    for idx in top_indices:
                        with st.expander(f"äº¤æ˜“ {idx+1} - æ¬ºè©æ¦‚ç‡: {predictions[idx]:.3f}"):
                            col1, col2 = st.columns(2)
                            
                            with col1:
                                st.write("**ç‰¹å¾µè²¢ç»:**")
                                for j, feature in enumerate(feature_names):
                                    try:
                                        if len(shap_values.shape) == 1:
                                            # 1D case - only one SHAP value per sample, distribute equally among features
                                            contribution = shap_values[idx] / len(feature_names)
                                        else:
                                            # 2D case - multiple features
                                            if j < shap_values.shape[1]:
                                                contribution = shap_values[idx, j]
                                            else:
                                                contribution = 0
                                        
                                        # Ensure contribution is a scalar value
                                        if hasattr(contribution, 'item'):
                                            try:
                                                contribution = contribution.item()
                                            except ValueError:
                                                # If item() fails, try to get the first element
                                                if hasattr(contribution, '__len__') and len(contribution) > 0:
                                                    contribution = contribution[0]
                                                else:
                                                    contribution = 0
                                        elif hasattr(contribution, '__len__') and len(contribution) > 0:
                                            contribution = contribution[0]
                                        
                                        # Convert to float for comparison
                                        try:
                                            contribution_float = float(contribution)
                                            color = "red" if contribution_float > 0 else "green"
                                        except (ValueError, TypeError):
                                            color = "black"
                                            contribution_float = 0
                                        
                                        st.write(f"â€¢ {feature}: <span style='color:{color}'>{contribution_float:.3f}</span>", 
                                                unsafe_allow_html=True)
                                    except Exception as e:
                                        st.write(f"â€¢ {feature}: Error - {str(e)}")
                            
                            with col2:
                                st.write("**åŸå§‹å€¼:**")
                                for j, feature in enumerate(feature_names):
                                    if j < shap_data['data'].shape[1]:
                                        value = shap_data['data'][idx, j]
                                        st.write(f"â€¢ {feature}: {value:.3f}")
                
                else:
                    st.warning("ç„¡æ³•ç”ŸæˆSHAPè§£é‡‹ï¼Œè«‹æª¢æŸ¥æ¨¡å‹ç‹€æ…‹")
                    
            except Exception as e:
                st.error(f"SHAPè§£é‡‹ç”Ÿæˆå¤±æ•—: {str(e)}")
                st.info("é€™å¯èƒ½æ˜¯ç”±æ–¼ç¼ºå°‘SHAPä¾è³´é …ã€‚è«‹é‹è¡Œ: pip install shap")
    
    with tab3:
        st.subheader("ğŸŒ åœ–ç¥ç¶“ç¶²çµ¡åˆ†æ")
        
        st.info("""
        **åœ–ç¥ç¶“ç¶²çµ¡åŠŸèƒ½:**
        - åˆ†æå¸³æˆ¶é–“çš„è³‡é‡‘æµå‹•æ¨¡å¼
        - æª¢æ¸¬ç•°å¸¸çš„ç¶²çµ¡çµæ§‹
        - è­˜åˆ¥æ½›åœ¨çš„æ´—éŒ¢ç¶²çµ¡
        - å¯¦æ™‚æ›´æ–°ç¶²çµ¡åµŒå…¥
        """)
        
        # Network statistics
        col1, col2, col3 = st.columns(3)
        
        with col1:
            unique_accounts = len(data['from_account'].unique())
            st.metric("å”¯ä¸€å¸³æˆ¶æ•¸", f"{unique_accounts:,}")
        
        with col2:
            total_transactions = len(data)
            st.metric("ç¸½äº¤æ˜“æ•¸", f"{total_transactions:,}")
        
        with col3:
            avg_connections = total_transactions / unique_accounts if unique_accounts > 0 else 0
            st.metric("å¹³å‡é€£æ¥æ•¸", f"{avg_connections:.1f}")
        
        # Network visualization placeholder
        st.subheader("ğŸ•¸ï¸ ç¶²çµ¡å¯è¦–åŒ–")
        st.info("ç¶²çµ¡å¯è¦–åŒ–åŠŸèƒ½æ­£åœ¨é–‹ç™¼ä¸­...")
        
        # Sample network data
        sample_accounts = data['from_account'].value_counts().head(10)
        
        fig = go.Figure(data=go.Bar(
            x=sample_accounts.values,
            y=sample_accounts.index,
            orientation='h',
            marker_color='rgba(255, 99, 132, 0.8)'
        ))
        
        fig.update_layout(
            title="æœ€æ´»èºå¸³æˆ¶ (å‰10å)",
            xaxis_title="äº¤æ˜“æ¬¡æ•¸",
            yaxis_title="å¸³æˆ¶ID",
            height=400
        )
        st.plotly_chart(fig, width='stretch')
    
    with tab4:
        st.subheader("âš¡ å¯¦æ™‚é æ¸¬åˆ†æ")
        
        # Real-time prediction interface
        st.write("**å¯¦æ™‚äº¤æ˜“é æ¸¬:**")
        
        # Sample recent transactions for prediction
        recent_data = data.tail(20)
        
        if st.button("ğŸ”„ æ›´æ–°é æ¸¬", width='stretch'):
            with st.spinner("æ­£åœ¨é€²è¡Œå¯¦æ™‚é æ¸¬..."):
                # Get predictions from hybrid AI
                predictions = models['hybrid_ai'].predict_proba(recent_data)
                
                # Create prediction results
                results_df = recent_data[['transaction_id', 'amount', 'timestamp']].copy()
                results_df['æ¬ºè©æ¦‚ç‡'] = predictions
                results_df['é¢¨éšªç­‰ç´š'] = results_df['æ¬ºè©æ¦‚ç‡'].apply(
                    lambda x: 'ğŸš¨ é«˜é¢¨éšª' if x >= 0.8 else 'âš ï¸ ä¸­é¢¨éšª' if x >= 0.5 else 'âœ… ä½é¢¨éšª'
                )
                
                # Display results
                st.dataframe(results_df, width='stretch')
                
                # Prediction statistics
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    high_risk = (predictions >= 0.8).sum()
                    st.metric("é«˜é¢¨éšªäº¤æ˜“", f"{high_risk}", f"{high_risk/len(predictions)*100:.1f}%")
                
                with col2:
                    medium_risk = ((predictions >= 0.5) & (predictions < 0.8)).sum()
                    st.metric("ä¸­é¢¨éšªäº¤æ˜“", f"{medium_risk}", f"{medium_risk/len(predictions)*100:.1f}%")
                
                with col3:
                    low_risk = (predictions < 0.5).sum()
                    st.metric("ä½é¢¨éšªäº¤æ˜“", f"{low_risk}", f"{low_risk/len(predictions)*100:.1f}%")
        
        # Model comparison
        st.subheader("ğŸ“Š æ¨¡å‹æ€§èƒ½æ¯”è¼ƒ")
        
        # Simulate model comparison
        models_comparison = pd.DataFrame({
            'æ¨¡å‹': ['å‚³çµ±éš¨æ©Ÿæ£®æ—', 'æ··åˆAIç³»çµ±', 'Transformer', 'åœ–ç¥ç¶“ç¶²çµ¡'],
            'æº–ç¢ºç‡': [0.945, 0.978, 0.962, 0.951],
            'ç²¾ç¢ºç‡': [0.923, 0.965, 0.948, 0.934],
            'å¬å›ç‡': [0.891, 0.942, 0.925, 0.908],
            'F1åˆ†æ•¸': [0.907, 0.953, 0.936, 0.921]
        })
        
        # Create comparison chart
        fig = go.Figure()
        
        metrics = ['æº–ç¢ºç‡', 'ç²¾ç¢ºç‡', 'å¬å›ç‡', 'F1åˆ†æ•¸']
        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']
        
        for i, metric in enumerate(metrics):
            fig.add_trace(go.Bar(
                name=metric,
                x=models_comparison['æ¨¡å‹'],
                y=models_comparison[metric],
                marker_color=colors[i]
            ))
        
        fig.update_layout(
            title="æ¨¡å‹æ€§èƒ½æ¯”è¼ƒ",
            xaxis_title="æ¨¡å‹",
            yaxis_title="åˆ†æ•¸",
            barmode='group',
            height=400
        )
        st.plotly_chart(fig, width='stretch')

# Footer
st.markdown("---")
st.markdown("""
<div style='text-align: center; padding: 20px;'>
    <h4>ğŸ›¡ï¸ è·¨å¢ƒæ”¯ä»˜æ¬ºè©æª¢æ¸¬ç³»çµ±</h4>
    <p>AIæ™ºæ…§ç¤¾æœƒç”±æ‚¨å‰µ - æ¾³é–€é›»è¨ŠAI+å¤§æ•¸æ“šæ™ºæ…§æ‡‰ç”¨è¨­è¨ˆæ¯”è³½</p>
    <p style='font-size: 0.9em; color: #666;'>
        Powered by Hybrid AI â€¢ Transformer â€¢ GNN â€¢ Meta-Learning â€¢ SHAP â€¢ Federated Learning â€¢ Behavioral Biometrics â€¢ Deepfake Detection
    </p>
    <p style='font-size: 0.8em; color: #999;'>
        æ•¸æ“šä¾†æº: Kaggle Credit Card Fraud Detection Dataset (284,807 transactions)
    </p>
</div>
""", unsafe_allow_html=True)
